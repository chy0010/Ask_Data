**Ask Data â€“ Retrieval-Augmented Generation (RAG) ğŸ“‚ğŸ¤–

Ask Data is a Retrieval-Augmented Generation (RAG) project that lets you query your documents and datasets in plain English. 
Instead of manually searching PDFs or writing SQL, you can just ask a question, and the system retrieves relevant information and generates a clear answer using a Large Language Model (LLM).

â¸»

âœ¨ Features
	â€¢	ğŸ” RAG-powered Q&A â€“ Combines retrieval + generation for accurate answers.
	â€¢	ğŸ“‘ Document Understanding â€“ Splits documents into chunks and retrieves only the relevant sections.
	â€¢	ğŸ§  Conversational Memory â€“ Supports follow-up questions with context retention.
	â€¢	âš¡ Fast Vector Search â€“ Embeddings stored in a vector DB for quick lookups.
	â€¢	ğŸ–¥ï¸ Notebook Interface â€“ Built in Jupyter for experimentation and learning.

â¸»

ğŸ› ï¸ Tech Stack
	â€¢	Python â€“ Core language
	â€¢	LangChain â€“ Orchestrates RAG pipeline (retriever + LLM)
	â€¢	Groq API â€“ LLaMA 70B â€“ LLM for response generation
	â€¢	ChromaDB â€“ Vector database for document embeddings
	â€¢	SentenceTransformers â€“ Creates embeddings for semantic similarity
	â€¢	PyPDF / NumPy / Pandas â€“ Data handling utilities

â¸»

âš™ï¸ RAG Workflow
	1.	Data Ingestion
	â€¢	Load documents (e.g., PDF, text).
	â€¢	Split into smaller chunks for embedding.
	2.	Embedding Creation
	â€¢	Convert each chunk into numerical vectors using SentenceTransformers.
	3.	Vector Store
	â€¢	Store embeddings in ChromaDB for similarity search.
	4.	Query & Retrieval
	â€¢	User enters a question â†’ converted into an embedding.
	â€¢	Retriever searches for the most relevant chunks.
	5.	Generation
	â€¢	Retrieved chunks are passed to LLaMA 70B via Groq API.
	â€¢	The LLM generates a contextual, human-like answer.
